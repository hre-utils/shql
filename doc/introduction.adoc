= baSH Query Language (shql) Basics
Marcus Aurelius <marcus [at] aurelius [dot] org>
:source-highlighter:     pygments
:pygments-style:         algol_nu
:pygments-linenums-mode: table
:toc:                    left
:toclevels:              3

A step-by-step introduction to understanding the Bash behind _shql_.

== The Objective
Parsing JSON into a bash readable data structure.

== Difficulties
The following are the general _bash specific_ difficulties to overcome.
There are other programming and design challenges, but those will be present in any language.


=== Everything is text
It is very often one hear the phrase `"everything is text"` in the *NIX world, especially in bash.
The upside is text processing languages (`awk`, `sed`) can interact with nearly anything,
and chaining a sequence of commands becomes very powerful.

It also makes (some) simple operations equally more difficult.

To fill gaps in the missing functionality, we need to abuse some of the exceptions to this rule.

==== Nesting
Bash _does_ have data types, which are declared with flags passed to the `declare` keyword.
Integers are declared with `-a`, indexed arrays with `-a`, and associative arrays with `-A`.
However, arrays cannot be nested.
You may only define flat dicts & lists.

.List
[source,bash]
----
declare -a indexed_array=(
   'item1'
   'item2'
)
----

.Dict
[source,bash]
----
declare -A associative_array=(
   [item1]='val1'
   [item2]='val2'
)
----

==== Arguments
Due to the text-based nature of Bash, passing arguments to functions can be difficult.
You cannot pass a list to a function as a parameter.
You can expand it as follows, which maintains the '`words`', but that leads to further complications.

[source,bash]
----
fn "${indexed_array[@]}"
----

If you want to pass two arrays, you'd need to additionally specify a preceding length for each array.
Or use another system to differentiate them.

Passing in an expanded associative array is sufficiently more rough.

=== Scope
There is basic variable scope within bash.
Using `declare` or `local` within a function defines a variable scoped locally to that function.
There are no file-scoped variables.

All other variables are global. Kinda.

=== Forks
There is oddness when running processes in a subshell (indicated by `(...)`, or `$(...)` to capture the output).

Subshells are forked, with a duplicate of your environment at the time of forking.
Any changes made within the subshell are local exclusively to that shell.
They are _not_ preserved or copied back when the process ends.

Thus, commands or functions that must be run within a subshell may not alter parameters of your script.
They cannot set variables, increment a counter, etc.

=== Classes
The traditional approach to building a lexer, or syntax tree, is class based.
One creates a class representing Tokens, or Tree Nodes, then creates instances of that class.

Bash does not have classes.
And due to the "`can't nest arrays`" issue, it's difficult to make a sorta pseudo class out of data + method lists.

== So... What Now?
Thankfully (or maybe not, depending who you ask) bash provides a wide range of creative solutions to address all the above problems.

=== Namerefs
A nameref is simply a pointer, or a reference, to another variable.
Modifying the nameref variable will act as if the operation was performed on the referenced variable.
A brief example:

[source,bash]
----
# Create regular string.
var='this is a string'

# Create reference to `var`, identified by `foo`.
declare -n foo=var

# Calling $foo is now identical to calling $var
echo $foo  # -> 'this is a string'
----

Namerefs are the absolute core of bash sneekiness and tomfoolery.
Passing two dicts to a function?
Easy.

[source,bash]
----
declare -A d1=( # ... )
declare -A d2=( # ... )

function fn {
   declare -n dict1=$1
   declare -n dict2=$2
   # ... do stuff here
}

fn 'd1' 'd2'
----

=== Nesting
When [ab]using namerefs, we're able to now nest arrays.
Simply create an array... of pointers to the new arrays.

[source,bash]
----
# Declare pointers to child arrays.
declare -a parent=( child1 child2 )

# Declare child arrays themselves.
declare -a child1=( 'child_var_1' 'child_var_2' )
declare -a child2=( 'child2_var_1' 'child2_var_2' )

# To traverse...
for child_array_name in "${parent[@]}" ; do
   echo "$child_array_name: "
   declare -n child_array=${parent[$child_array_name]}
   for child_var in "${child_array[@]}" ; do
      echo "  $child_var"
   done
done

# The above prints:
#> child1
#>   child_var_1
#>   child_var_2
#> child2
#>   child2_var_1
#>   child2_var_2
----

=== Data Types
Neato burrito.
Now we can make references to variables by their names, but we need to know the type ahead of time.
What if the parent level list contains pointers to integers, associative arrays, strings, and more indexed arrays?
No problem.
Create function(s) to handle printing each type.
Determine the type of the variable the nameref points to.
Call its associated print function.

[source,bash]
----
function print_by_type {
   local var_name=$1
   local var_type=$( declare -p $var_name | awk '{print $2' )

   # The above is a bit of silliness. `declare -p` will display the attributes
   # and value of a variable in the following format:
   #
   #> $ var='this'
   #> $ declare -p var
   #> declare -- var="this"
   #
   # By awking the 2nd word ('--'), we can determine the type of the variable
   # based on its flags.

   case $var_type in
      '--')  print_string ;;
      '-a')  print_list   ;;
      '-A')  print_dict   ;;
      *) raise_unhandled_type_error
   esac
}
----

== Structure
With namerefs out of the way, creating tokens and syntax tree nodes feels sufficiently more approachable.

=== Lexing
Bash's built-in support for text-based pattern matching makes lexing straightforward.

I'll be keeping the computer science relatively terse.
Both because I want to minimize the length of this article, and because I can't claim to understand it very well.
Lexing, in brief, is iterating character by character through an input file, and categorizing sequences of text into "Tokens".

For example, this short piece of text...

[source,text]
----
let var='val'
----

\... may generate the following Tokens:

[source]
----
Token(type: 'DECLARATION', 'value': 'let')
Token(type: 'IDENTIFIER',  'value': 'var')
Token(type: 'STRING',      'value': 'val')
----

Tokens allow us to express a structure for how data should _look_, as well as holding useful metadata (location in the file, location in the line, etc.).
Creating a bare bones lexer in bash requires a few pieces.

==== Pointers
Due the scoping of subshells, we cannot call a function that modifies state, _and_ returns an output value.
An easy way to circumvent this is global pointers.

As we iterate through the text file, `$CURRENT` is updated to always refer to the current character, while `$PEEK` is the subsequent one.
Some operations require checking one or two characters ahead.
For example, if `$CURRENT` currently is an `=`, is it an '`EQUALS`' Token?
Not necessarily.
It's dependent on the next character: maybe a following `>` makes it a '`GREATER_LESS_THAN`'.

Some Tokens can be quite long, such as strings.
Upon hitting an opening quotation mark, we must seek 'til the closing quote, meanwhile appending all data to the `$BUFFER`.
The Token's value is set to the `$BUFFER` contents, and it is reset.

To ensure the uniqueness of our Token names, a global integer (`$TOKEN_NUM`) is declared.
It is incremented before the instantiation of each token, ensuring we don't stomp on previous ones.

==== Tokens
A Token is no more than a globally defined dictionary, which is appended to the `TOKENS` array.
A basic Token creation function looks like this:

[source,bash]
----
function Token {
      (( ++TOKEN_NUM ))

      # Create unique token *name*, append to TOKENS list.
      local token_name="TOKEN_${TOKEN_NUM}"
      TOKENS+=( $token_name )

      # Create global token itself, and a local nameref
      declare -Ag $token_name
      declare -n  token=$token_name

      # Assign data based passed in from calling function.
      token[type]="$1"
      token[value]="$2"
}
----

==== The Lexer
Thus, a very basic lexer:

[source,bash]
----
function lex {
   # To make iterating tokens w/ lookahead a little easier, first read the file
   # contents into an array. This allows for trivial seek in either direction.
   declare -ag CHARRAY
   while read -rN1 c ; do
      CHARRAY+=( "$c" )
   done < "$INPUT_FILE"

   while [[ -n $PEEK ]] ; do
      advance
      # Little function that steps us through the ${CHARRAY[@]}, and sets
      # $CURRENT/$PEEK values to the current/next characters.

      # Skip whitespace.
      [[ "$CURRENT" =~ [[:space:]] ]] && continue

      # Symbols.
      case "$CURRENT" in
         ';')  TOKEN   'SEMI' "$CURRENT" &&  continue ;;
         ':')  TOKEN  'COLON' "$CURRENT" &&  continue ;;
         '.')  TOKEN 'PERIOD' "$CURRENT" &&  continue ;;
         # ...
      esac

      # Strings.
      if [[ "$CURRENT" =~ [\"\'] ]] ; then
         string "$CURRENT"
         # ^-- Nums all characters, after the initial quote, until it hits the
         # closing quotation. The intermediate characters are added to the
         # string $BUFFER.

         TOKEN 'STRING' "$BUFFER"
      fi

      # If we've hit a character that's not covered by our lexer, it's invalid.
      # We'll want to log that somehow. For the scope of this project, it's
      # sufficient to add an ERROR token to the token stream, which can be
      # caught at/before the parser. This approach allows us to collect *all*
      # potential errors in lexing, rather than blowing up at the first one.
      TOKEN 'ERROR' "$CURRENT"
   done
}
----

The end result will be a series of globally defined Tokens, whose names will be stored sequentially in the `$TOKENS` array.

=== Parsing
Parsing is sufficiently more complex to explain given the intent of this document.
I will do my best, but '`real`' parsing is nearing the limit of my CS knowledge.

==== Background
We need to turn our list of Tokens into something meaningful.
Intrinsically they have no significance, nor can any operations be performed upon them.
The parser, following a deterministic set of rules, translates Tokens into a "`syntax tree`".

The type of ruleset I've used is a sort of https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form[EBNF], a type of notation to describe context-free grammars.
In a stupidly short amount of detail...
There are nonterminal rules, and terminal symbols.
The following describes basic arithmetic operations.

[source]
----
expression  -> term ((PLUS|MINUS) term)*
term        -> factor ((MUL|DIV) factor)*
factor      -> NUMBER
             | '(' expression ')'
             | (PLUS|MINUS) factor
----

This can be read as: "`An epression is defined as a '`term`', followed by 0 or more '`plus`' or '`minus`' operations and another '`term`'.`"
"`A term is defined as a '`factor`', followed by either a '`multiplication`' or 'division`' operation and another '`factor`'.`", etc.
Note that a factor can also be an opening parenthesis, followed by a brand new expression statement, then a closing parenthesis.
This is evaluated recursively.
Given the input `4 + 3 * 2`, it could be interpreted as...

[source]
----
expression(term(factor(4)), '+', term(factor(3), '*', factor(2)))
----

The syntax used above should give a hint as to how we translate Tokens -> Tree.
Create functions named for each of the above grammar rules, '`expression`', '`term`', '`factor`'.
Each function will check if the current Token matches it's expected type.
If so, it munches that token, stores the value, and continues.
The `expression()` function would call `term()`, then if the subsequent Token's type was `PLUS` or `MINUS`, it would munch that one, then call `term()` again.

==== The Parser
In Bash, we cannot generate the requisite AST nodes, for the same reason we can't create true Tokens.
So our Classes look like the following:

[source,bash]
----
function mkString {
   ((GLOBAL_AST_NUMBER++))
   local node_name="_NODE_${GLOBAL_AST_NUMBER}"
   declare -g $node_name
   declare -g AST_NODE="$node_name"
}


function mkList {
   ((GLOBAL_AST_NUMBER++))
   local node_name="_NODE_${GLOBAL_AST_NUMBER}"
   declare -ga $node_name
   declare -g AST_NODE="$node_name"
}


function mkDictionary {
   ((GLOBAL_AST_NUMBER++))
   local node_name="_NODE_${GLOBAL_AST_NUMBER}"
   declare -gA $node_name
   declare -g AST_NODE="$node_name"
}
----

We use the same trick as in lexing to generate unique tokens.
The global pointer `$AST_NODE` references the name of the last created node.
From within functions, we can use this pointer to add Token data into our nodes.
To declare a string, then add data into it:

[source,bash]
----
function grammar_string {
   mkString
   declare -- node_name=$AST_NODE
   declare -n node=$node_name

   node="${TOKEN[data]}"
}
----

We call `mkString()`, then define a nameref to the created node.
If this function has been called, the current Token is a string.
Then assign the text of the string (the `Token[data]`) to the node.
A list will be slightly more complicated.

[source,bash]
----
function grammar_list {
   mkList
   declare -- node_name=$AST_NODE
   declare -n node=$node_name

   # Requires at least one item. No empty lists here!
   grammar_data
   node+=( $AST_NODE )

   # If there's more data, continue to append.
   while [[ ${PEEK[type]} == 'COMMA' ]] ; do
      munch 'COMMA'
      grammar_data
      node+=( $AST_NODE )
   done

   munch 'R_BRACKET'

   # Reset global AST pointer to this List node.
   AST_NODE=$node_name
}
----

Initially create a List, and save a local reference to the name of that AST node, as well as a nameref to the list itself.
Lists contain data, which may be strings, dicts, or further lists.
The `grammar_data()` function will create a corresponding node for the subsequent Tokens datatype.
`$AST_NODE` has been updated now to the generated node from `grammar_data()`.
This is appended to the List.
So long as the subsequent Token is a comma, we continue to call `grammar_data()`, and append data nodes to our List.
Once we've run out of data, munch the closing bracket.
Then we return the global `$AST_NODE` pointer to its value prior to calling the `grammar_list()` function.

==== The Result
We've now created a bunch of nodes, but how do we _get_ them?
Simple.
Using the same `declare -p` trick as before to dump the declaration.
Combined with `"${!_NODE_*}"`, which expands to all variables whose names begin with '`_NODE_`'.

[source,bash]
----
declare -p "${!_NODE_*}" > output.sh
----

Now, given the following input JSON:
[source,json]
----
{
   "here": [
      "one",
      "two"
   ],
   "this": "that",
   "more": {
      "foo": "bar"
   }
}
----

We are now rewarded with parsed Bash:
[source,bash]
----
declare -A _NODE_1=([here]="_NODE_3" [this]="_NODE_2" [more]="_NODE_6" )
declare -- _NODE_2="that"
declare -a _NODE_3=([0]="_NODE_4" [1]="_NODE_5")
declare -- _NODE_4="one"
declare -- _NODE_5="two"
declare -A _NODE_6=([foo]="_NODE_7" )
declare -- _NODE_7="bar"
----


=== Uhhh
We've successfully read the input JSON, lexed into Tokens, parsed into a bash-compatible representation.
So we're done right?

Uhh, no.

Now that the data is IN Bash, we'll need some means of _doing_ something with it.
That is, performing standard operations upon the whole (or subsets of) the data.
"`Print this section`", "`delete that key`", etc..
Time to repeat the entire process above, but for a query language of our own design.

=== Queries
=== Lexing, Again
=== Parsing, Again
