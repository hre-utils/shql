#!/bin/bash
# changelog
#  2021-07-24  :: Created
#
# desc :: starting with some real quick and dirty stuff here to see if this
#         is going to work at all--attempt is to make a basic, but featureful,
#         .json lexer & parser in bash. It shall be .shon. Shell object
#         notation.
# todo :: 1) Change all CURRENT's to be the namerefs themselves, and give all
#            dicts a [name] prop.

#══════════════════════════════════╡ GLOBAL ╞═══════════════════════════════════
# Color garbage.
rst=$(tput sgr0)                                   # Reset
bk="$(tput setaf 0)"                               # Black
rd="$(tput setaf 1)"  ;  brd="$(tput bold)${rd}"   # Red     ;  Bright Red
gr="$(tput setaf 2)"  ;  bgr="$(tput bold)${gr}"   # Green   ;  Bright Green
yl="$(tput setaf 3)"  ;  byl="$(tput bold)${yl}"   # Yellow  ;  Bright Yellow
bl="$(tput setaf 4)"  ;  bbl="$(tput bold)${bl}"   # Blue    ;  Bright Blue
mg="$(tput setaf 5)"  ;  bmg="$(tput bold)${mg}"   # Magenta ;  Bright Magenta
cy="$(tput setaf 6)"  ;  bcy="$(tput bold)${cy}"   # Cyan    ;  Bright Cyan
wh="$(tput setaf 7)"  ;  bwh="$(tput bold)${wh}"   # White   ;  Bright White


INFILE="$1"
if [[ ! -e "$INFILE" ]] ; then
   echo "Missing input file"
   exit 1
fi


# For later, doing incremental backups on each change.
PROGDIR=$( cd "$(dirname "${BASH_SOURCE[0]}")" ; pwd )
BACKUP1="${PROGDIR}/.backup1.json"
BACKUP2="${PROGDIR}/.backup2.json"
BACKUP3="${PROGDIR}/.backup3.json"

#──────────────────────────────────( lexing )───────────────────────────────────
declare -a TOKENS=()
declare -i GLOBAL_TOKEN_NUMBER=0 IDX=0

declare -a CHARRAY=()
declare -A CURSOR=(
   [lineno]=1
   [colno]=0
   [pos]=-1
   # Kinda dumb and hacky. Starting at (-1) so the first call to advance() will
   # increment by 1, thus reading the *next* character, the first.
)

declare -A FREEZE
declare -- CURRENT PEEK BUFFER

#──────────────────────────────────( parsing )──────────────────────────────────
declare -A _ROOT=()

# Same dumb nonsense here as in 11k. Starting at -1 so initial advance is to 0.
declare -i tIDX=-1
declare -- TOKEN tNAME tPEEK1 tPEEK2
# tNAME is the tname of the current token.
# TOKEN is a nameref to the token itself.

# AST generation
declare -- AST_NODE
declare -i GLOBAL_AST_NUMBER=0

#──────────────────────────────────( output )───────────────────────────────────
declare -i INDNT_SPS=2     # How many spaces per level of indentation
declare -i INDNT_LVL=0     # Current level of indentation


#═══════════════════════════╡ PRINTING & DEBUGGING ╞════════════════════════════
declare -A colormap=(
   [DOT]="$yl"
   [COLON]="$wh"
   [COMMA]="$wh"
   [STRING]="$rd"
   [COMMENT]="$cy"
   [L_BRACE]="$wh"
   [R_BRACE]="$wh"
   [L_BRACKET]="$wh"
   [R_BRACKET]="$wh"
   [EOF]="$gr"
)

function print_tokens {
   for tname in "${TOKENS[@]}" ; do
      declare -n t="$tname"
      declare col="${colormap[${t[type]}]}"
      printf "${col}%-10s${rst}  ${bk}%-7s${rst}  ${col}${t[data]}${rst}\n" \
         ${t[type]} \
         "${t[lineno]}:${t[colno]}"
   done
}


#═══════════════════════════════════╡ LEXER ╞═══════════════════════════════════
function Token {
   ttype="$1"
   data="${2:-$BUFFER}"

   if [[ -z "$ttype" ]] ; then
      echo "Missing \$ttype" ; exit 2
   fi

   # Make token. Add to list.
   tname="token_${GLOBAL_TOKEN_NUMBER}"
   TOKENS+=( $tname )

   declare -Ag $tname
   declare -n t=$tname

   # Data.
   t[type]="$ttype"
   t[data]="$data"

   # Meta information.
   t[lineno]=${FREEZE[lineno]}
   t[colno]=${FREEZE[colno]}
   t[pos]=${FREEZE[pos]}

   # Increment.
   ((GLOBAL_TOKEN_NUMBER++))

   # TODO: Hitting an odd situation in which the first increment is returning a
   #       '1' status for some reason. Need to figure out why that is. Remove
   #       elements to narrow down. I don't think incrementing a number should
   #       return anything but '0'.
   return 0
}


function advance {
   # Advance position in file, and position in line.
   ((CURSOR[pos]++))
   ((CURSOR[colno]++))

   CURRENT=
   PEEK=

   if [[ ${CURSOR[pos]} -lt ${#CHARRAY[@]} ]] ; then
      CURRENT=${CHARRAY[CURSOR[pos]]}
   fi

   if [[ ${CURSOR[pos]} -lt $((${#CHARRAY[@]}-1)) ]] ; then
      PEEK=${CHARRAY[CURSOR[pos]+1]}
   fi

   if [[ "$CURRENT" == $'\n' ]] ; then
      ((CURSOR[lineno]++))    # Increment line number.
      CURSOR[colno]=0         # Reset column position.
   fi
}


function comment {
   while [[ -n $CURRENT ]] ; do
      [[ "$PEEK" =~ [$'\n'] ]] && break
      advance
   done
}


function string {
   delim="$1"
   declare -a buffer=()

   while [[ -n $CURRENT ]] ; do
      if [[ "$PEEK" =~ [$delim] ]] ; then
         if [[ "${buffer[-1]}" == '\\' ]] ; then
            unset buffer[-1]
         else
            break
         fi
      fi
      advance
      buffer+=( "$CURRENT" )
   done

   # Set global buffer to joined output of local buffer.
   # >>> BUFFER = ''.join(local_buffer)
   BUFFER=''
   for c in "${buffer[@]}" ; do
      BUFFER+="$c"
   done

   # Create token.
   Token 'STRING'

   # Skip final closing ('|").
   advance
}


function identifier {
   BUFFER="$CURRENT"

   while [[ -n $CURRENT ]] ; do
      [[ "${PEEK}" =~ [^[:alnum:]_] ]] && break
      advance ; BUFFER+="$CURRENT"
   done

   #Token 'IDENTIFIER'
   local loc="[${FREEZE[lineno]}:${FREEZE[colno]}]"
   Token 'ERROR'  "Syntax Error: $loc 'Identifier' not yet implemented."
}


function lex {
   # Fill into array, allows us to seek forwards & backwards.
   while read -rN1 c ; do
      CHARRAY+=( "$c" )
   done < "$INFILE"

   # Iterate over array of characters. Lex into tokens.
   while [[ ${CURSOR[pos]} -lt ${#CHARRAY[@]} ]] ; do
      advance
      [[ -z $CURRENT ]] && break

      # "Freeze" the line number and cursor number, such that they're attached
      # to the *start* of a token, rather than the end.
      FREEZE[lineno]=${CURSOR[lineno]}
      FREEZE[colno]=${CURSOR[colno]}
      FREEZE[pos]=${CURSOR[pos]}

      # Skip comments.
      if [[ "$CURRENT" == '#' ]] ; then
         comment ; continue
      fi

      # Skip whitespace.
      [[ "$CURRENT" =~ [[:space:]] ]] && continue

      # Symbols.
      case "$CURRENT" in
         '.')  Token       'DOT' "$CURRENT" &&  continue ;;
         ':')  Token     'COLON' "$CURRENT" &&  continue ;;
         ',')  Token     'COMMA' "$CURRENT" &&  continue ;;
         '{')  Token   'L_BRACE' "$CURRENT" &&  continue ;;
         '}')  Token   'R_BRACE' "$CURRENT" &&  continue ;;
         '[')  Token 'L_BRACKET' "$CURRENT" &&  continue ;;
         ']')  Token 'R_BRACKET' "$CURRENT" &&  continue ;;
      esac

      # Strings.
      if [[ "$CURRENT" =~ [\"\'] ]] ; then
         string "$CURRENT" ; continue
      fi

      # Identifiers.
      if [[ "$CURRENT" =~ [[:alpha:]_] ]] ; then
         identifier ; continue
      fi

      local loc="[${FREEZE[lineno]}:${FREEZE[colno]}]"
      Token 'ERROR'  "Syntax Error: $loc Invalid character ${CURRENT@Q}."
   done

   Token 'EOF' 'null'
}

#══════════════════════════════════╡ PARSER ╞═══════════════════════════════════
# What is the grammar?
#     data   -> (dict|list|STRING)
#     dict   -> '{' STRING COLON data [COMMA dict]* '}'
#     list   -> '[' data [COMMA data]* ']'
#
# Do we need to create tree nodes, or can we parse directly into lists & dicts?
# I think we need to make a tree of *names* first.
# Then based on our name tree, we can make functions that can either query, or
# pretty print the underlying data.
#
# Is this realistically making a compiler? I think so. What people would now
# refer to as a "transpiler".
#
# Need to do some thinkies.
# Given the following object:
# {
#    "this": "that",
#    "dict": {
#       "child": "value",
#       "child2": "value"
#    },
#    "list": [
#       "one", "two", "three"
#    ]
# } 
# 
# Should create the following data structures
#  declare -A ROOT_NODE=(
#     
#  )
#
# Everything always starts with the ROOT_NODE, which may be either a string,
# list, or dict. Depending on the type we'll have a function that creates the
# corresponding data type, and inserts the value(s).

# May need a function like the one below when parsing to either read a list of
# values, or key/value pairs. Don't know yet.
#function get_type {
#   [[ $(declare -p $1) =~ declare\ (.*) $1 ]]
#   
#   case "${BASH_REMATCH[1]//[-]}" in
#      'a')  echo 'LIST' ;;
#      'A')  echo 'DICT' ;;
#      '--') echo 'STRING' ;;
#   esac
#}
#
# How did it work before...
# Series of functions, each with arguments to pull from its data dict, or
# children dict. Children dict maps friendly name to FQfn. Data dict is simple
# key:value. Are functions the best way to do this, or should we try using a
# dict-based approach? Advantages of each:
# Dicts are potentially faster to call, though we need to write a function to
# recursively descend through them. Function based approach has the leg up in
# terms of *being* the function the handles the recursion.
# Going to try the dict-based approach here, as it's not what I used in the last
# tree.
#
# How do we know if something is a pointer to another nested structure, vs. a
# value itself. Maybe just a `declare -p`? Doesn't work if the user defines a
# value matching an identifier used in this script. Need to explicitly
# separate the raw values from the children.
#declare -A _ROOT__d__=(
#   [this]="that"
#)
#declare -A _ROOT__c__=(
#   [dict]=_ROOT__dict
#   [list]=_ROOT__dict
#)

# We want the dictionary to explicitly state that ROOT.foo == _ROOT__foo, because
# it may be an identifier that's resolved later. E.g., _ROOT__IDENT_bar?
# I need to think this through more.

# At this point need to work on creating the AST nodes via the method above.
# Remember. Explicit is better than implicit.


function raise_parse_error {
   local loc="[${TOKEN[lineno]}:${TOKEN[colno]}]"
   printf "Parse Error: ${loc} "
   printf "Expected ${1}, received ${TOKEN[type]}.\n"

   exit -2
}


function raise_key_error {
   local loc="[${TOKEN[lineno]}:${TOKEN[colno]}]"
   printf "Key Error: ${loc} "
   printf "Key ${1@Q} invalid. Must be a valid 'word'.\n"

   exit -3
}


function t_advance {
   # Advance position in file, and position in line.
   ((tIDX++)) 

   TOKEN=
   tNAME=
   tPEEK1=
   tPEEK2=

   if [[ ${tIDX} -lt ${#TOKENS[@]} ]] ; then
      tNAME=${TOKENS[tIDX]}
      declare -gn TOKEN=${tNAME}
   fi

   # Lookahead +1.
   if [[ ${tIDX} -lt $((${#TOKENS[@]}-1)) ]] ; then
      declare t1=${TOKENS[tIDX+1]}
      declare -gn tPEEK1=$t1
   fi

   # Lookahead +2.
   if [[ ${tIDX} -lt $((${#TOKENS[@]}-2)) ]] ; then
      declare t2=${TOKENS[tIDX+2]}
      declare -gn tPEEK2=$t2
   fi
}


function mkDictionary {
   local node_name="_NODE_${GLOBAL_AST_NUMBER}"

   declare -gA $node_name
   declare -gn AST_NODE="$node_name"

   ((GLOBAL_AST_NUMBER++))
}


function mkList {
   local node_name="_NODE_${GLOBAL_AST_NUMBER}"

   declare -ga $node_name
   declare -gn AST_NODE="$node_name"

   ((GLOBAL_AST_NUMBER++))
}


function mkString {
   echo -n #pass
}


function check_lex_errors {
   declare -a errors_found=()

   for tname in "${TOKENS[@]}" ; do
      declare -n t="$tname"
      if [[ ${t[type]} == 'ERROR' ]] ; then
         errors_found+=( "${t[data]}" )
      fi
   done

   if [[ ${#errors_found[@]} -gt 0 ]] ; then
      for e in "${errors_found[@]}" ; do
         echo "$e"
      done
      exit -1
   fi
}


function munch {
   t_advance
   declare -a expected=( ${1//,/ } )

   local found=false
   for exp in "${expected[@]}" ; do
      if [[ ${TOKEN[type]} == $exp ]] ; then
         found=true ; break
      fi 
   done

   $found || raise_parse_error "${expected[@]}"
}


function parse {
   check_lex_errors

   grammar_data
   munch 'EOF'
}


#──────────────────────────────────( grammar )──────────────────────────────────
function grammar_data {
   munch 'STRING,L_BRACE,L_BRACKET'

   case ${TOKEN[type]} in
      'STRING')      grammar_string ;;
      'L_BRACE')     grammar_dict   ;;
      'L_BRACKET')   grammar_list   ;;
      *) raise_parse_error  ;;
   esac
}


function grammar_string {
   echo -n # pass
}


function grammar_dict {
   ((INDNT_LVL++))

   mkDictionary
   declare -n d=AST_NODE

   munch 'STRING'
   munch 'COLON' 
   grammar_data

   while [[ ${tPEEK1[type]} == 'COMMA' ]] ; do
      munch 'COMMA'
      munch 'STRING'
      munch 'COLON'
      grammar_data
   done

   munch 'R_BRACE'
   ((INDNT_LVL--))
}


function grammar_list {
   ((INDNT_LVL++))

   mkList
   declare -n l=AST_NODE

   grammar_data

   while [[ ${tPEEK1[type]} == 'COMMA' ]] ; do
      munch 'COMMA'
      grammar_data
   done
   
   munch 'R_BRACKET'
   ((INDNT_LVL--))
}


lex
parse
#print_tokens
