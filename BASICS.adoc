= baSH Query Language (shql) Basics
Marcus Aurelius <marcus [at] aurelius [dot] org>
:source-highlighter:     pygments
:pygments-style:         algol_nu
:pygments-linenums-mode: table
:toc:                    left
:toclevels:              2

A step-by-step introduction to understanding the Bash behind _shql_.

== The Objective
Parsing JSON into a bash readable data structure.

== Difficulties
The following are the general _bash specific_ difficulties to overcome.
There are other programming and design challenges, but those will be present in any language.


=== Everything is text
It is very often one hear the phrase `"everything is text"` in the *NIX world, especially in bash.
The upside is text processing languages (`awk`, `sed`) can interact with nearly anything,
and chaining a sequence of commands becomes monstrously powerful.

It also makes (some) simple operations equally monstrously more difficult.

To fill gaps in the missing functionality, we need to abuse some of the exceptions to this rule.

==== Nesting
Bash _does_ have data types.
Integers may also be declared (`declare -i int`).
There are indexed arrays (`declare -a list`), and associative arrays (`declare -A dict`).
However, arrays cannot be nested. You may only define flat dicts & lists.

.List
[source,bash]
----
declare -a indexed_array=(
   'item1'
   'item2'
)
----

.Dict
[source,bash]
----
declare -A associative_array=(
   [item1]='val1'
   [item2]='val2'
)
----

==== Arguments
Due to the text-based nature of Bash, passing arguments to functions can be difficult.
You cannot pass a list to a function as a parameter.
You can expand it as follows, which maintains the '`words`', but that leads to further complications.

[source,bash]
----
fn "${indexed_array[@]}"
----

If you want to pass two arrays, you'd need to additionally specify a preceding length for each array.
Or use another system to differentiate them.

Passing in an expanded associative array is sufficiently more rough.

=== Scope
There is basic variable scope within bash.
Using `declare` or `local` within a function defines a variable scoped locally to that function.
There are no file-scoped variables.

All other variables are global. Kinda.

=== Forks
There is oddness when running processes in a subshell (indicated by `(...)`, or `$(...)` to capture the output).

Subshells are forked, with a duplicate of your environment at the time of forking.
Any changes made within the subshell are local exclusively to that shell.
They are _not_ preserved or copied back when the process ends.

Thus, commands or functions that must be run within a subshell may not alter parameters of your script.
They cannot set variables, increment a counter, etc.

=== Classes
The traditional approach to building a lexer, or syntax tree, is class based.
One creates a class representing Tokens, or Tree Nodes, then creates instances of that class.

Bash does not have classes.
And due to the "`can't nest arrays`" issue, it's difficult to make a sorta pseudo class out of data + method lists.

== So... What Now?
Thankfully (or maybe not, depending who you ask) bash provides a wide range of creative solutions to address all the above problems.

=== Namerefs
A nameref is simply a pointer, or a reference, to another variable.
Modifying the nameref variable will act as if the operation was performed on the referenced variable.
A brief example:

[source,bash]
----
# Create regular string.
var='this is a string'

# Create reference to `var`, identified by `foo`.
declare -n foo=var

# Calling $foo is now identical to calling $var
echo $foo  # -> 'this is a string'
----

Namerefs are the absolute core of bash sneekiness and tomfoolery.
Passing two dicts to a function?
Easy.

[source,bash]
----
declare -A d1=( # ... )
declare -A d2=( # ... )

function fn {
   declare -n dict1=$1
   declare -n dict2=$2
   # ... do stuff here
}

fn 'd1' 'd2'
----

=== Nesting
When [ab]using namerefs, we're able to now nest arrays.
Simply create an array... of pointers to the new arrays.

[source,bash]
----
# Declare pointers to child arrays.
declare -a parent=( child1 child2 )

# Declare child arrays themselves.
declare -a child1=( 'child_var_1' 'child_var_2' )
declare -a child2=( 'child2_var_1' 'child2_var_2' )

# To traverse...
for child_array_name in "${parent[@]}" ; do
   echo "$child_array_name: "
   declare -n child_array=${parent[$child_array_name]}
   for child_var in "${child_array[@]}" ; do
      echo "$child_var"
   done
done

# The above prints:
#> child1
#> child_var_1
#> child_var_2
#> child2
#> child2_var_1
#> child2_var_2
----

=== Data Types
Neato burrito.
Now we can make references to variables by their names, but we need to know the type ahead of time.
What if the parent level list contains pointers to integers, associative arrays, strings, and more indexed arrays?

No problem.

Create function(s) to handle printing each type.
Determine the type of the variable the nameref points to.
Call its associated print function.

[source,bash]
----
function print_by_type {
   local var_name=$1
   local var_type=$( declare -p $var_name | awk '{print $2' )

   # The above is a bit of silliness. `declare -p` will display the attributes
   # and value of a variable in the following format:
   #
   #> $ var='this'
   #> $ declare -p var
   #> declare -- var="this"
   #
   # By awking the 2nd word ('--'), we can determine the type of the variable
   # based on its flags.

   case $var_type in
      '--')  print_string ;;
      '-a')  print_list   ;;
      '-A')  print_dict   ;;
      *) raise_unhandled_type_error
   esac
}
----

== Structure
With namerefs out of the way, creating tokens and syntax tree nodes feels sufficiently more approachable.

=== Lexing
Bash's built-in support for text-based pattern matching makes lexing straightforward.

I'll be keeping the computer science relatively terse.
Both because I want to minimize the length of this article, and because I can't claim to understand it very well.
Lexing, in brief, is iterating character by character through an input file, and categorizing sequences of text into "Tokens".

For example, this short piece of text...

[source,text]
----
let var='val'
----

\... may generate the following Tokens:

[source]
----
Token(type: 'DECLARATION', 'value': 'let')
Token(type: 'IDENTIFIER',  'value': 'var')
Token(type: 'STRING',      'value': 'val')
----

Tokens allow us to express a structure for how data should _look_.

Creating a bare bones lexer in bash requires a few pieces.

==== Pointers
Due the scoping of subshells, we cannot call a function that modifies state, _and_ returns an output value.
An easy way to circumvent this is global pointers.

As we iterate through the text file, `$CURRENT` is updated to always refer to the current character, while `$PEEK` is the subsequent one.
Some operations require checking one or two characters ahead.
For example, if `$CURRENT` currently is an `=`, is the token type _EQUALS_?
Not necessarily.
It's dependent on the next character: maybe a `>`, or even a second `=`.

Some Tokens can be quite long, such as strings.
Upon hitting an opening quotation mark, we must seek 'til the closing quote, meanwhile appending all data to the `$BUFFER`.
The Token's value is set to the `$BUFFER` contents, and it is reset.

To ensure the uniqueness of our Token names, a global integer (`$TOKEN_NUM`) is declared,.
It is incremented before the instantiation of each token, ensuring we don't stomp on previous ones.

==== Tokens
A Token is no more than a globally defined dictionary, which is appended to the `TOKENS` array.
A basic Token creation function looks like this:

[source,bash]
----
function Token {
      (( ++TOKEN_NUM ))

      # Create unique token *name*, append to TOKENS list.
      local token_name="TOKEN_${TOKEN_NUM}"
      TOKENS+=( $token_name )

      # Create global token itself, and a local nameref
      declare -Ag $token_name
      declare -n  token=$token_name

      # Assign data based passed in from calling function.
      token[type]="$1"
      token[value]="$2"
}
----

==== The Lexer
Thus, a very basic lexer:

[source,bash]
----
function lex {
   # To make iterating tokens w/ lookahead a little easier, first read the file
   # contents into an array. This allows for trivial seek in either direction.
   declare -ag CHARRAY
   while read -rN1 c ; do
      CHARRAY+=( "$c" )
   done < "$INPUT_FILE"

   while [[ -n $PEEK ]] ; do
      advance
      # Little function that steps us through the ${CHARRAY[@]}, and sets
      # $CURRENT/$PEEK values to the current/next characters.

      # Skip whitespace.
      [[ "$CURRENT" =~ [[:space:]] ]] && continue

      # Symbols.
      case "$CURRENT" in
         ';')  TOKEN   'SEMI' "$CURRENT" &&  continue ;;
         ':')  TOKEN  'COLON' "$CURRENT" &&  continue ;;
         '.')  TOKEN 'PERIOD' "$CURRENT" &&  continue ;;
         # ...
      esac

      # Strings.
      if [[ "$CURRENT" =~ [\"\'] ]] ; then
         string "$CURRENT"
         # ^-- Nums all characters, after the initial quote, until it hits the
         # closing quotation. The intermediate characters are added to the
         # string $BUFFER.

         TOKEN 'STRING' "$BUFFER"
      fi

      # If we've hit a character that's not covered by our lexer, it's invalid.
      # We'll want to log that somehow. For the scope of this project, it's
      # sufficient to add an ERROR token to the token stream, which can be
      # caught at/before the parser. This approach allows us to collect *all*
      # potential errors in lexing, rather than blowing up at the first one.
      TOKEN 'ERROR' "$CURRENT"
   done
}
----

The end result will be a series of globally defined Tokens, whose names will be stored sequentially in the `$TOKENS` array.

=== Parsing
Parsing is sufficiently more complex to explain given the intent of this document.
I will do my best, but '`real`' parsing operations are nearing the limit of my CS knowledge.

We need to turn our list of Tokens into something meaningful.
Intrinsically they have no significance, nor can any operations be performed upon them.
The parser, following a deterministic set of rules, translates Tokens into a "`syntax tree`".

%% *CURRENT* %%


=== Uhhh
We've successfully read our input JSON, lexed it into Tokens, and parsed it into a bash-compatible representation.
So we're done right?

Uhh, no.

Now that the data is IN Bash, we'll need some means of querying it.
That is, performing standard operations upon the whole (or subsets of) the data.
"`Print this section`", "`delete this key`", etc..
Time to repeat the entire process above, but for a query language of our own design.

=== Queries
=== Lexing, Again
=== Parsing, Again
